Extreme Computing
=================

* Name: Christopher Brown
* Matriculation Number: 0824586

Task One
--------

The first task is to write a simple Naive Bayes classifier for the
capitalisation of words. The first thing that I do is loop through each
of the training words and split it up into it's features. I made the
decision to remove any duplicates from the features list in the case of
short words with 2 or 3 letters as this would give unfair weighting to
features that were more common in short words. This could have had
undesirable side effects as many of these short words were acronyms
which are almost always upper case.

After the word had been split into features I counted the occurrences of
the upper case and lower case features based on their containing word.
These values were then stored in a map. Once this counting was complete
we iterate through this map and calculate the probabilities of the
classes based on the features.

After we have built this model we can start iterating through the test
document words, extracting their features, and multiplying together the
product of their features before choosing the class with the largest
product to classify the word. Once the word is classified it is a simple
matter of manipulating it to conform with its class definition.

This simple classifier achieves 84.2672413793103% accuracy when trained
with the small file.

Task Two
--------

### Mapper

The mapper in this case is responsible for taking it's input lines,
splitting them up into words, calculating the features for those words
(with the same considerations as above) and emitting those values to
be processed in the reducer. The format that I decided on using for the
emitted data from the mapper is as follows:

    <feature> <uppercase> <count>

The first element of the tuple is the string of the feature that is
being emitted. The second element is a 0 or a 1 depending on whether or
not the feature is from an lower or upper case word respectively. The
final value in the tuple is a count of how many times this configuration
of case and feature have occurred. In my code this is always emitted as
1 but the reducer was designed to accept any numerical value here to
allow the mapper to be easily modified to store local counts temporarily
and emit them in batches.

### Reducer

The reducer has a slightly more complex job to do as it must contain
some state as to what is the current feature being processed along with
the counts of how many occurrences of the lower and upper class there
have been. On every line of input it checks to see if there is a new
feature present and if not it updates the class counts before reading in
the next line. If the new feature is different then we output the
probabilities of the current feature before starting the process again
with the new feature.

The reducer outputs the following format:

    <feature> <lower case probability> <upper case probability>

The entire flow of data was tested using Unix pipes and redirection.
Multiple test files were sent to the mapper before being sorted and sent
on to the reducer. The string of commands to achieve this is as follows:

    cat testfile | ruby mapper.rb | sort | ruby reducer.rb

The test files that were used were deliberately obtuse and aimed to find
edge cases.

Task Three
----------

### Mapper

Task three is relatively simple. At a high level it takes in words from
the test data and breaks them up into their features. The output format
of the mapper is:

    <word> <feature>

I elected to use this format rather than having all of the features in
one line due to the ease of sorting. We would be able to confidently
sort on the feature if we know that all of the emitted values will have
the same format with the same number of entries in the tuple. In fact,
we do exactly this later in Part 4.

### Reducer

The reducer performs the simple task of making sure that we do not
repeat a line in the output. Due to the fact that there is probably
duplicate words in the input then the mapper is going to emit duplicate
versions of the word-feature tuples. The reducer stops these duplicate
values being carried on to the next stage of the classification.

It uses a Bloom Filter to make accurate guesses whether or not
a particular line had been seen before. When a line is detected we first
check to see if it is in the Bloom Filter. If it has not been seen then
we add it to the array of seen lines and the Bloom Filter before
outputting it. If the incoming line *is* present in the Bloom Filter
then we perform the more expensive check (O(n) vs. O(1)) as to whether
or not the line has definitely been seen by checking to see if it is in
the array or not. If it has not then we output it and add it to the
array.

Task Four
---------

Task Four is the first step that requires more than one set of mappers
and reducers in my implementation.

### First Mapper

The first mapper's responsibility is to make sure that everything to do
with a certain feature goes to the same reducer for the next stage of
computation. This is more challenging than it seems because we have two
different inputs to this mapper. Luckily, they are in two different
formats and so we can easily differentiate between them. The two
different formats are:

    <word> <feature>

and:

    <feature> <lower case probability> <upper case probability>

In order to make these go to the same mapper we need to make sure they
have the same key - the feature. In the first case we just reverse the
tuple before emitting it and in the second case we can just emit it
straight away. We aren't able to this split using secondary sorting due
to the two different formats of data that are being emitted from the
mapper.

### First Reducer

The first reducer's job is to take the various forms of input from the
mapper above and merge them into a consistent and useful format for
later use. In this case the format is:

    <word> <feature> <lower probability> <upper probability>

This contains all of the information that the next stage of the
computation needs to classify the word.

The code for this stage is quite complex because it needs to be able to
handle many different orders and permutations of input order due to the
many-to-many relationship from words to features.

### Second Mapper

For the second mapper I used the `cat` Unix command as no work needed to
be done except that the input to the reducer needed to be sorted which
would be carried out by Hadoop in the interim between mapper and
reducer. I decided to use `cat` for two main reasons:

1. `cat` has been taking input from STDIN and outputting it to STDOUT
   for, quite literally, longer that I have been alive. It's provably
   reliable and robust.
2. It is a compiled executable and will be able to output lines faster
   than anything I would have been able to write.

### Second Reducer

The second reducer receives the tuples from the first stage sorted by
word. From this it is a simple task to multiply together the
probabilities until a new word is reached in the input and then
outputting a new tuple based on the maximum product value. This reducer
outputs tuples of the form:

    <word> <upper>

Where <word> is the string of the word to classify and <upper> is the
classification of the word (0 for lower, 1 for upper).

Task Five
---------

### Mapper

The mapper in Task 5 simply takes the tuple from Task 4 and manipulates
the string accordingly before outputting it.

Because of the nature of Hadoop and more generally the Map-Reduce
programming paradigm the output of this step is completely unsorted and
has lost any of the duplicates that were present in the original test
text. To check that we have the same output as the Unix version of the
classifier we can simply check to see that if we sort and remove
duplicates of the output of the Unix classifier that we get the same file
as the output from Hadoop. We are able to do this because there is no
possibility of Naive Bayes taking one instance of a word and classifying
it differently as another instance of the word and as such the sorted,
de-duplicated lists will be the same and so the classifiers return the
same result.

Code
====

Part 1
------

### bayes.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # Returns the different lower case features of the word.
    #
    # These are:
    #
    # * The word itself.
    # * The first 2 letters of the word.
    # * The first 3 letters of the word.
    # * The last 2 letters of the word.
    # * The last 3 letters of the word.
    #
    def features(word)
      [
        word,
        word[0..1],
        word[0..2],
        word[-2..-1],
        word[-3..-1]
      ].map do |w|
        w.downcase unless w.nil?
      end.compact
    end

    # Takes a file and returns a list of all of the words that are in the file.
    def parse_file(path)
      File.new(path).read.split
    end

    # Show some help if the user gets it wrong.
    unless ARGV.length == 2
      puts "Usage: #{$0} [training-file] [testing-file]"
      exit 1
    end

    # Create some lists of the training and testing words.
    TRAINING_WORDS = parse_file(ARGV[0])
    TESTING_WORDS = parse_file(ARGV[1])

    # Create a hash with a default value that has empty counts.
    words = Hash.new do |hash, key|
      hash[key] = Array.new(2, 0)
    end

    # Loop through each of the training words and increment the counts for each
    # feature of the word.
    TRAINING_WORDS.each do |word|
      # Does the word have any capital letters in it?
      uppercase = word.match(/^[A-Z]/) ? 1 : 0
      word_features = features(word)

      word_features.each do |feature|
        words[feature][uppercase] += 1
      end
    end

    # Let's create a new hash to store the probabilities of the case given the
    # feature of the word.
    probabilities = Hash.new do |hash, key|
      hash[key] = Array.new(2, 0)
    end

    # And then we'll loop through our counts to create the model for using with
    # Naive Bayes.
    words.each_key do |key|
      sum = words[key].inject(:+)

      (0..1).each do |i|
        value = words[key][i].to_f/sum
        probabilities[key][i] = value
      end
    end

    # Okay, so we have our model. Lets get capitalising some words! This function
    # takes a word and then words out if it should be upper or lower case. If it
    # should be upper case then it returns 1 and if lower case then a 0. It also
    # takes the model that it should base these predicions off.
    #
    def word_case(word, model)
      word_features = features(word)

      # The probabilities of each feature being lower or upper case.
      lower_prob = []
      upper_prob = []

      # Retrieve the probabilities.
      word_features.each do |feature|
        # If we've never seen this feature before in our model.
        if !model.has_key?(feature)
          lower_prob << 1
          upper_prob << 1
          next
        end

        lower_prob << model[feature][0]
        upper_prob << model[feature][1]
      end

      # Calculate the product of them.
      lower = lower_prob.inject(:*)
      upper = upper_prob.inject(:*)

      # Return the class of the larger one.
      upper > lower ? 1 : 0
    end

    # For each of the words in the test data we now classify it and print it out.
    TESTING_WORDS.each do |word|
      # Should it be upper case?
      upper = word_case(word, probabilities) > 0

      # Capitalise the word if it should be uppercase
      word.capitalize! if upper

      # We're done! Output the word.
      puts word
    end

Part 2
------

### run.sh

    #/usr/bin/env sh

    hadoop fs -rmr /user/s0824586/coursework/output-part2

    hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar \
                  -jobconf mapred.job.name="s0824586 - Part 2" \
                  -input /user/s0824586/coursework/input-train-small \
                  -output /user/s0824586/coursework/output-part2 \
                  -mapper mapper.rb \
                  -file mapper.rb \
                  -reducer reducer.rb \
                  -file reducer.rb

### mapper.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # Returns the different features of the word.
    #
    # These are:
    #
    # * The word itself.
    # * The first 2 letters of the word.
    # * The first 3 letters of the word.
    # * The last 2 letters of the word.
    # * The last 3 letters of the word.
    #
    def features(word)
      [
        word,
        word[0..1],
        word[0..2],
        word[-2..-1],
        word[-3..-1]
      ].map do |w|
        w.downcase unless w.nil?
      end.compact
    end

    # Naive Bayes
    #
    # Loop through the words in the input and split them up into their features. We
    # then emit a tuple with the feature, whether or not the word is uppercase or
    # not, and the count. The count may just be 1 or we could cache some of these
    # in the mapper and send them in blocks.
    #
    ARGF.each do |line|
      words = line.chomp.split

      words.each do |word|
        uppercase = word.match(/[A-Z]/) ? 1 : 0
        word_features = features(word)

        word_features.each do |feature|
          puts "#{feature}\t#{uppercase}\t1"
        end
      end
    end

### reducer.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # Create a hash with a default value that has empty counts.
    words = Hash.new do |hash, key|
      hash[key] = Array.new(2, 0)
    end

    # Naive Bayes
    #
    # Loop through each tuple in the input and keep a count of how many times it
    # has appeared in upper and lower case words.
    #
    # Once the sequence of the same word is finished then we output the
    # probabilities of the words being upper or lower case in the format:
    #
    # <feature><tab><lower probability><tab><upper probability>
    #
    current_feature = nil

    ARGF.each do |line|
      feature, uppercase, count = line.chomp.split

      # If we have reached a new word in the input then output all of the current words.
      unless feature == current_feature
        words.each_key do |key|
          sum = words[key][0] + words[key][1]
          lower = words[key][0].to_f/sum
          upper = words[key][1].to_f/sum

          puts "#{key}\t#{lower}\t#{upper}"
        end

        # Set the new feature to be the current one.
        current_feature = feature

        # Clear out the existing words.
        words.clear
      end

      # Change the strings to an integers.
      uppercase = uppercase.to_i
      count = count.to_i

      words[feature][uppercase] += count
    end

    # Remember to output the last word in the file.
    words.each_key do |key|
      sum = words[key][0] + words[key][1]
      lower = words[key][0].to_f/sum
      upper = words[key][1].to_f/sum

      puts "#{key}\t#{lower}\t#{upper}"
    end

Part 3
------

### run.sh

    #/usr/bin/env sh

    # Delete the output folder
    hadoop fs -rmr /user/s0824586/coursework/output-part3

    # Run the task
    hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar \
                  -jobconf mapred.job.name="s0824586 - Part 3" \
                  -input /user/s0824586/coursework/input-test \
                  -output /user/s0824586/coursework/output-part3 \
                  -mapper mapper.rb \
                  -file mapper.rb \
                  -reducer reducer.rb \
                  -file reducer.rb

### mapper.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # Returns the different features of the word.
    #
    # These are:
    #
    # * The word itself.
    # * The first 2 letters of the word.
    # * The first 3 letters of the word.
    # * The last 2 letters of the word.
    # * The last 3 letters of the word.
    #
    def features(word)
      [
        word,
        word[0..1],
        word[0..2],
        word[-2..-1],
        word[-3..-1]
      ].map do |w|
        w.downcase unless w.nil?
      end.compact
    end

    ARGF.each do |line|
      # Splot the line into words
      words = line.chomp.split

      # Output the features of each word.
      words.each do |word|
        word_features = features(word)

        word_features.each do |feature|
          puts "#{word}\t#{feature}"
        end
      end
    end

### reducer.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # Simple bloom filter implementation
    #
    # The hashing is dreadful and wrong but it works well enough.
    class BloomFilter
      def initialize(size)
        @size = size
        @buffer = Array.new(size, false)
      end

      def insert(element)
        hash(element).each { |i| @buffer[i] = true}
      end

      def maybe_include?(element)
        hash(element).map { |i| @buffer[i] }.inject(:&)
      end

      private :hash
      def hash(element)
        letters = element.chars.map { |e| ?e }

        [
          letters.inject(:+),
          letters.inject(:*)
        ].map { |h| h % @size }
      end
    end

    bloom_filter = BloomFilter.new(2000)
    used_words = []

    # We want to make sure that the features of a word are only outputted once
    # even though there may be multiple occurences of the word in the text.
    #
    # We use a bloom filter to check if we have seen the line before. This prevents
    # the O(n) check of the inclusion of the line in an used_words array. Since
    # a bloom filter is not 100% accurate we must check the array if the bloom filter
    # thinks we have seen the line before.
    ARGF.each do |line|
      if !bloom_filter.maybe_include?(line)
        puts line
        bloom_filter.insert(line)
        used_words << line
      else
        unless used_words.include? line
          puts line
          used_words << line
        end
      end
    end

Part 4
------

### run.sh

    #/usr/bin/env sh

    # Delete the output folder
    hadoop fs -rmr /user/s0824586/coursework/output-part4-step-1
    hadoop fs -rmr /user/s0824586/coursework/output-part4-step-2

    # Run the task
    hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar \
                  -jobconf mapred.job.name="s0824586 - Part 4 - Step 1" \
                  -input /user/s0824586/coursework/output-part2 \
                  -input /user/s0824586/coursework/output-part3 \
                  -output /user/s0824586/coursework/output-part4-step-1 \
                  -mapper mapper-step1.rb \
                  -file mapper-step1.rb \
                  -reducer reducer-step1.rb \
                  -file reducer-step1.rb \

    # Run the task
    hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar \
                  -jobconf mapred.job.name="s0824586 - Part 4 - Step 2" \
                  -input /user/s0824586/coursework/output-part4-step-1 \
                  -output /user/s0824586/coursework/output-part4-step-2 \
                  -mapper cat \
                  -reducer reducer-step2.rb \
                  -file reducer-step2.rb \

### mapper-step1.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # There are two different formats of data that will be coming into this mapper.
    #
    # Feature Probabilities:
    #   <feature> <lower probability> <upper probability>
    #
    # Word Features:
    #   <word> <feature>
    #

    ARGF.each do |line|
      tokens = line.split

      case tokens.length
      when 2
        puts tokens.reverse.join "\t"
      when 3
        puts line
      end
    end

### reducer-step1.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # This reducer is responsible for creating tuples of the form:
    #
    #   <word> <feature> <upper probability> <lower probability>
    #
    # For further processing in the next stage.
    #

    current_feature = {
      :name => nil,
      :lower => 0,
      :upper => 0
    }

    saved_words = []

    # Output the current list of saved words to STDOUT.
    def dump_saved_words(saved_words, feature)
      return if feature[:lower].nil?

      saved_words.each do |word|
        puts "#{word}\t#{feature[:name]}\t#{feature[:lower]}\t#{feature[:upper]}"
      end
    end

    ARGF.each do |line|
      tokens = line.split

      if tokens.length == 2
        feature, word = tokens

        # If this is a new feature then we should reset the state of the
        # script and output all of the saved words.
        if feature != current_feature[:name]
          dump_saved_words(saved_words, current_feature)
          saved_words.clear
          current_feature = {
            :name => feature,
            :lower => nil,
            :upper => nil
          }
        end

        saved_words << word
      else
        feature, lower, upper = tokens

        # If this is a new feature then we should reset the state of the
        # script and output all of the saved words.
        if feature != current_feature[:name]
          dump_saved_words(saved_words, current_feature)
          saved_words.clear
        end

        # Set the current feature to the new one.
        current_feature = {
          :name => feature,
          :lower => lower,
          :upper => upper
        }
      end
    end

    # Make sure that the last word in the file is also outputted.
    dump_saved_words(saved_words, current_feature)

### reducer-step2.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    current_word = nil
    lower_product = 1
    upper_product = 1

    # This script takes lines of format:
    #
    #   <word> <feature> <lower probability> <upper probability>
    #
    # It then takes all of the values for a word and outputs the following:
    #
    #   <word> <0 or 1>
    #
    # The second value is 0 if the word should be lower case and 1 if the word
    # should be upper case.
    ARGF.each do |line|
      word, feature, lower, upper = line.split
      current_word = word if current_word.nil?

      unless current_word == word
        is_upper = upper_product > lower_product
        puts "#{current_word}\t#{is_upper ? 1 : 0}"

        current_word = word
        upper_product = 1
        lower_product = 1
      end

      lower_product *= lower.to_f
      upper_product *= upper.to_f
    end

    # Make sure we output the last word.
    is_upper = upper_product > lower_product
    puts "#{current_word}\t#{is_upper ? 1 : 0}"

Part 5
------

### run.sh

    #/usr/bin/env sh

    # Delete the output folder
    hadoop fs -rmr /user/s0824586/coursework/output-part5

    # Run the task
    hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar \
                  -jobconf mapred.job.name="s0824586 - Part 5" \
                  -input /user/s0824586/coursework/output-part4-step-2 \
                  -output /user/s0824586/coursework/output-part5 \
                  -mapper mapper.rb \
                  -file mapper.rb \
                  -reducer cat

    rm ../output/*
    hadoop fs -copyToLocal /user/s0824586/coursework/output-part5/* ../output
    cat ../output/* > ../output-new

### mapper.rb

    #!/usr/bin/env ruby

    # Make sure that Ruby doesn't try and `help` us by buffering the output at all.
    STDOUT.sync = true

    # Really simple script that takes a tuple and uppercases the first
    # element if the second element is 1.
    ARGF.each do |line|
      word, upper = line.split
      upper = (upper == "1")

      word.capitalize! if upper

      puts word
    end
